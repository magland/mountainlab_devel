%&latex
\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{algpseudocode}
\usepackage{algorithm}


%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Unimodal clustering using isotonic regression: ISO-SPLIT}
  \author{Jeremy F. Magland\hspace{.2cm}\\
    Simons Center for Data Analysis\\
    and \\
    Alex H. Barnett \\
    Simons Center for Data Analysis \\ and Department of Mathematics, Dartmouth College}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Unimodal clustering using isotonic regression: ISO-SPLIT}
  \end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
A limitation of many clustering algorithms is the requirement to tune adjustable parameters for each application or even for each dataset. Some algorithms require an \emph{a priori} estimate of the number of clusters while density-based techniques usually require a scale parameter. Other parametric methods, such as mixture modeling, make assumptions about the underlying cluster distributions. Here we introduce a non-parametric clustering method that does not involve tunable parameters and only assumes that clusters are unimodal, in the sense that they have a single point of maximal density when projected onto any line, and that clusters are separated from one another by a separating hyperplane of relatively lower density. The technique uses a non-parametric algorithm---isotonic regression---as the kernel operation repeated at every iteration. We carry out a rigorous hypothesis test for whether pairs of clusters should be merged based upon Monte Carlo sampling of a statistic. We compare the method against k-means++, DBSCAN, and Gaussian mixture algorithms and show in simulations that it performs better than these standard methods in many situations. The algorithm's utility is also demonstrated in the context of ``spike sorting'' of neural electrical recordings. The source code for the algorithm is freely available.
\end{abstract}

\noindent%
{\it Keywords:}  non-parametric, spike sorting, density-based clustering, Monte Carlo
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\section {Introduction}

Unsupervised data clustering is a methodology for automatically partitioning a set of data points in a manner that reflects the underlying structure of the data. In many clustering applications with continuous data in $n$ dimensions, clusters are expected to have a core region of high density and to be separated from one another by a region of relatively lower density. The motivating application for the authors is spike sorting of neuron firing events recorded electrically, for which this property has been found to hold experimentally \cite{tiganj,vargas}. We expect this work to be useful in a variety of other clustering applications.

A limitation of most clustering algorithms is the need to tune a set of adjustable parameters. The adjustments may be per application, or even per dataset. For k-means \cite{kmeans}, the adjustable parameter is $K$, the prospectively estimated number of clusters. For large datasets where dozens of clusters are present, the choice of $K$ is especially problematic. In addition, the output of k-means depends heavily on the initialization step (choosing seed points), and the algorithm is often repeated several times to obtain a more globally optimal solution. K-means++ \cite{kmeanspp} does a better job at seeding, but some rerunning is still required. Even with optimal seeding, if some clusters are small or sparse relative to the dominant cluster then they are often merged into nearby clusters. In general, k-means tends to favor artificially splitting larger clusters at the expense of merging smaller ones. A further limitation is that the algorithm assumes isotropic cluster distributions with equal populations and equal variances.

Gaussian mixture modeling (GMM), usually solved using expectation-maximization (EM) iterations \cite{em}, is more flexible than k-means since it allows each cluster to be assigned its own multivariate normal distribution. Many variations exist \cite[Ch.~11]{murphy}. While some implementations require prospective knowledge of the number of clusters (e.g., \cite[Ch.~8]{mixturemodels}), other implementations consider this as a free variable (e.g., \cite{roberts1998bayesian}). The key limitation is that the algorithm assumes that clusters are Gaussian distributed. Furthermore, as in k-means, it can be difficult to find the global solution, especially when the number of clusters is large.

Hierarchical clustering \cite[Ch.~14]{zaki-book} does not require specification of the number of clusters ahead of time, but this is because the output is a dendrogram rather than a partition. Thus it cannot immediately be applied to our application of interest. There is a way to obtain an automated partition from the dendrogram, but this involves specifying a criteria for cutting the binary tree (much like specifying $K$). Other choices need to be made for agglomerative hierarchical clustering in order to determine which clusters are merged at each iteration. Furthermore, hierarchical clustering has time complexity at least $O(N^2)$ where $N$ is the size of the dataset \cite[Sec.~14.2.3]{zaki-book}.

Density-based clustering techniques such as DBSCAN \cite{dbscan} are promising since they do not make assumptions about
data distributions %(e.g., multivariate gaussian)
so can handle clusters with arbitrary non-convex shapes. The drawback is that two parameters must be adjusted depending on the application, including $\epsilon$, a parameter of scale. The algorithm is especially sensitive to this parameter in higher dimensions. A further limitation is that if the clusters
substantially differ in density, then no choice of $\epsilon$ will simultaneously handle the entire dataset. Thus a scale-independent method using data density is desirable.

Other density-based techniques, such as the mean-shift algorithm \cite{mean-shift}, involve the initial step of constructing a continuous non-parametric probability density function \cite[Ch.~15]{zaki-book}. The basic version of the kernel density method \cite{kernel-density-function-1,kernel-density-function-2} involves specifying a spatial scale parameter (the so-called bandwidth), and suffers from the same problem as DBSCAN. Variations of this method can automatically estimate an optimal bandwidth \cite{silverman-density-estimation}, and can even derive a value that is spatially dependent. There are many density-estimation methods to choose from (including \cite{rodriguez-clustering}), but they often depend on adjustable distance parameters.
%such as the coarseness of the grid.
In general, these methods become computationally intractable in higher dimensions. One such algorithm that has a freely available R implementation is pdfCluster \cite{pdfcluster}. For even moderately sized datasets, computation time can be prohibitive, as will be shown experimentally, particularly when the number of dimensions is greater than 2 or 3.

Here we introduce an efficient density-based, scale-independent clustering technique suited for situations where clusters are expected to be unimodal and when any pair of distinct clusters may be separated by a hyperplane. We say a cluster is {\em unimodal} if it arises from a distribution that has a single point of maximum density when projected onto ony line. Thus, our assumption is that the projection of any two clusters onto the normal of the dividing hyperplane gives a bimodal distribution separating the clusters at its local minimum. Loosely speaking, this is guaranteed when the clusters are sufficiently spaced and have convex shapes.

In addition to being density-based, our algorithm has the flavor of agglomerative hierarchical clustering as well as the EM-style iterative approach of k-means. The algorithm uses a modified version of ``isotonic regression,'' \cite{pava}
a non-parametric method for fitting a data series by a monotonically increasing/decreasing curve. Most notably, isotonic regression involves no adjustable parameters, thus no scale parameter is needed for density estimation. Furthermore, since the core step performed at each iteration is one-dimensional (1D) clustering applied to projections of data subsets onto lines, we avoid the curse of dimensionality (the tradeoff being that we cannot handle clusters of arbitrary shape).

This paper is organized as follows. First we describe an algorithm for splitting a 1D sample into unimodal clusters based on a statistical test that uses isotonic regression and no adjustable parameters. This algorithm forms the basis of the $n$-dimensional clustering technique, ISO-SPLIT, defined in Section 3. Simulation results are presented in Section 4, comparing ISO-SPLIT with three standard clustering techniques. In addition to quantitative comparisons using a measure of accuracy, examples illustrate situations where each algorithm performs best. The fifth section is an application of the algorithm to spike sorting of neuronal data. Next we describe Monte Carlo methods for calibrating the core statistical test of the one-dimensional algorithm and discuss computational efficiency and scaling properties. Finally, Section 8 summarizes the results and discusses the limitations of the method. The appendices cover implementation details for isotonic regression, generation of synthetic datasets for simulations, and provide evidence for insensitivity to parameter adjustments.

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{illustration_1d.eps}
\end{center}
\caption{
(A,B,C) Histograms for three simulated 1D distributions with $N=1000$ datapoints. The first represents a single cluster whereas the second and third are sampled from bimodal probability distributions. (D,E,F) show the corresponding log densities at each point, computed as described in the text. The dark curves are the outputs of updown isotonic regression (defined in Section \ref{clustering_1d} and Appendix \ref{appendixUpdown}) for approximating the series of log densities. The sharp spikes are expected artifacts of the fitting algorithm (explained in the text). (G,H,I) show the corresponding residuals that are fit using downup isotonic regression. The wider dips in the dark curves of (H) and (I) indicate that the set of samples should be split into two clusters, whereas the narrow dip in (G) is not statistically significant.
}
\label{fig:plots_1d}
\end{figure}

\section {Clustering in one dimension using isotonic regression}
\label{clustering_1d}

Any algorithm overcoming the above limitations must at least be able to do so in the simplest, 1D case ($n=1$). Here we develop a non-parametric approach to 1D clustering using a variant of isotonic regression. The procedure will then be used as the kernel operation in the more general situation ($n\geq2$) as described in Section~\ref{isosplit-algorithm}.

Clustering in 1D is unique because the input data can be sorted. The problem reduces to selecting cutpoints (real numbers between adjacent data points) that split the data into $K$ clusters, each corresponding to an interval on the real line. We assume that the clusters are unimodal so that adjacent clusters are separated by a region of relatively lower density. For simplicity we will describe an algorithm for deciding whether there is one cluster, or more than one cluster. In the latter case, a single cutpoint is determined representing the boundary separating one pair of adjacent clusters. Note that once the data have been split, the same algorithm may then be applied recursively on the left and right portions leading to further subdivisions, converging when no more splitting occurs. Thus the algorithm described here may be used as a basis for general 1D clustering.

Let $x_1<\dots<x_N$ be the sorted (assumed distinct\footnote{The data are assumed to be independent samples from a continuous probability distribution, thus distinct with probability one. More details are given in the discussion.}) real numbers (input data samples). Our fundamental assumption is that two adjacent clusters are always separated by a region of lower density. In other words, if $a_1$ and $a_2$ are the \emph{centers} of two adjacent 1D clusters, then there exists a cut point $a_1<c<a_2$ such that the density near $c$ is significantly less than the densities near both $a_1$ and $a_2$. The challenge is to define the notion of density near a point. The usual way is to choose a neighborhood of size $\epsilon$ and to assign the density near $a$ as the number of data points within an $\epsilon$-neighborhood of $a$ divided by $\epsilon$ (similar to binning for a histogram). However, as described above, we want to avoid choosing a length scale $\epsilon$.

Instead we define the preliminary density at $x_j$ to be $d_j=2/(x_{j+1}-x_{j-1})$ for $j=2,\dots,M-1$, $d_1=1/(x_2-x_1)$, $d_M=(x_M-x_{M-1})$ (we assume that the $x_j$ are all distinct). This is essentially equivalent to nearest neighbor density estimation \cite[Ch.~15]{zaki-book}. In this paper we work with the log density $D_j=\log(d_j)$ so that scale adjustments correspond to additive shifts. While $D_j$ is a natural scale-independent estimate of the pointwise density, it is of course very noisy (see D,E,F of Figure~\ref{fig:plots_1d}). This is because, in the case where the data points are distributed uniformly near $x_j$, $1/d_j$ is a gamma distributed random variable with shape parameter $2$ (average of two exponential random variables). Typically the $k$ nearest neighbors are used (for $k$ much larger than $2$) to prevent excessive local maxima in the estimated density. However, we want to avoid such tunable parameters.

The null hypothesis in our statistical test is that the set $\{x_j\}$ forms a single unimodal cluster. To make this precise, we assume that the samples are drawn independently from a distribution with probability density function $f(x)$. We say that this distribution represents a single cluster if $f$ has no local minima, or in other words that it is monotonically increasing from $x=-\infty\text{ to }x_b$, where $x_b$ is a point of peak density, and then monotonically decreasing from $x=x_b\text{ to }\infty$. Since, up to a proportionality constant, $d_j$ is an estimate of $f$ near $x_j$, we want to test whether $D_1,\dots,D_N$ may be well approximated by $\tilde{D}_1,\dots,\tilde{D}_N$ where $\tilde{D}_1\leq \tilde{D}_2\leq \dots\leq \tilde{D}_b$ and $\tilde{D}_b\geq \tilde{D}_{b+1}\geq\dots \geq \tilde{D}_N$ for some $b$. A least-squares optimal choice for $\tilde{D}$ may be obtained using \emph{updown isotonic regression}, a variant of isotonic regression with a detailed explanation in Appendix \ref{appendixUpdown}. 

Let $E_j=D_j-\tilde{D}_j$ be the residual density where $\tilde{D}_j$ is the least-squares best approximation to $D_j$ satisfying the monotonicity contstraint defined above. If the data represent a single cluster, then the series $E_1,\dots,E_N$ should not systematically deviate from zero. On the other hand, if the data represent two or more clusters, then $D_j$ will not be well-approximated by an increasing-then-decreasing sequence, and hence $E_1,\dots,E_N$ will have a systematic dip at the regions of lower density between clusters. This is illustrated in Figure~\ref{fig:plots_1d}(H,I) where the dip is modest but sustained over a large width.

To detect such a dip (and hence to detect the presence of more than one cluster) we use \emph{downup} isotonic regression (similar to \emph{updown}) for approximating $E_j$ by $\tilde{E}_j$ where the latter sequence is monotonically decreasing and then monotonically increasing. Again, this procedure has no adjustable parameters. We then consider the values of $\tilde{E}_j$ near its minimum. Low values of $\tilde{E}$ correspond to areas of low density suggesting multiple clusters, see the dark lines in Figure~\ref{fig:plots_1d}(G,H,I). Note that sharp peaks in the updown and downup fits always occur at the transition point between increasing and decreasing since there is no constraint preventing an exact fit of this value. To improve the performance of the algorithm, based on empirical experiments, a weighting is applied during downup isotonic regression, with weights equal to $\sqrt{d_j^{-1}}$ (See Appendix~\ref{appendixUpdown}).

A novel feature of our algorithm is that we use a rigorous statistical test for whether there is one or more unimodal clusters, as follows.
In cases where two clusters are separated by a single large gap as in Figure~\ref{fig:plots_1d}(C), it is sufficient to consider one or two values of $\tilde{E}_j$ at its minimum, see Figure~\ref{fig:plots_1d}(I). On the other hand, in other cases it is necessary to compute the average of $\tilde{E}_j$ over a larger set of points in order to achieve statistical significance. This happens when there is a modest dip in density that is sustained over many samples, see Figure~\ref{fig:plots_1d}(B,H). Let $\tilde{E}_{j_1},\dots,\tilde{E}_{j_N}$ be the sorted version of $\tilde{E}$ in ascending order. We introduce the following test statistic:
$$
\label{eq:test_statistic}
\eta(\tilde{E})=\max_{m=1,\dots,m_\text{max}} \frac{\mu_{N,m}-A_m(\tilde{E})}{\sigma_{N,m}},
$$
where
$$A_m(\tilde{E})=\frac{1}{m}\sum_{k=1}^m \tilde{E}_{j_k},$$
and $\mu_{N,m}$ and $\sigma_{N,m}$ are the expected value and standard deviation, respectively, for $A_m(\tilde{E})$ when $x$ is sampled from a uniform distribution. Note that $A_m$ is simply the mean of the lowest $m$ values of $\tilde{E}$. The purpose of dividing by $\sigma_{N,m}$ is to normalize the values in the $A_m$ sequence so they are comparable before taking the maximum. Here $m_\text{max}$ is a dataset-independent parameter that just needs to be chosen sufficiently large (see Appendix~\ref{appendixSensitivity}).

The values $\mu_{N,m}$ and $\sigma_{N,m}$ are not computed on the fly, but are stored in a two-dimensional lookup table (using linear interpolation in $N$). These are pre-estimated using Monte Carlo simulations for each $N$ of interest. This is described in a Section~\ref{monte-carlo}. Monte Carlo simulations may again be used to convert $\eta(\tilde{E})$ to a $p$-value to test whether $\eta(\tilde{E})$ is greater than expected under the null hypothesis. When $p<\alpha$, we reject the null hypothesis and conclude that the sample represents more than one cluster ($\alpha$ is a chosen significance level, 0.1 in this paper, see discussion below). The cutpoint is then $x_j$ where $j$ minimizes $\tilde{E}_j$.

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{decision_boundaries.eps}
\end{center}
\caption{
Placement of decision boundaries for ISO-SPLIT is more flexible than for k-means clustering. In ISO-SPLIT, the assumption is that any two clusters are separated by a hyperplane. Unlike in k-means, which assumes all clusters have the same variance, this hyperplane may be positioned closer to one of the centroids than the other.
}
\label{fig:decision_boundaries}
\end{figure}

\section {Clustering in higher dimensions using one-dimensional projections}

\label{isosplit-algorithm}

In this section we address the $n$-dimensional situation ($n\geq2$) and describe an iterative procedure, termed ISO-SPLIT, in which the 1D algorithm is repeated as a kernel operation. The decision boundaries are less restrictive than $k$-means which always splits space into Voronoi cells with respect to the centroids, as illustrated in Figure~\ref{fig:decision_boundaries}.


The proposed procedure is outlined in Algorithm~\ref{alg:main_algorithm}. The input is a collection of $N$ points in $\mathbb{R}^n$, and the output is the collection of corresponding labels (or cluster memberships). The approach is similar to agglomerative hierarchical methods in that we start with a large number of clusters (output of \emph{InitializeLabels}) and iteratively reduce the number of clusters until convergence. However, in addition to merging clusters the algorithm may also redistribute data points between adjacent clusters. This is in contrast to agglomerative hierarchical methods. At each iteration, the two \emph{closest} clusters (that have not yet been handled) are selected and all data points from the two sets are projected onto a line orthoganal to the proposed hyperplane of separation. The 1D split test from the previous section is applied (see above) and then the points are redistributed based on the optimal cutpoint, or if no statistically significant cut point is found the clusters are merged. This procedure is repeated until no pair of unhandled clusters remains.

\spacingset{0.95} % Change spacing for algorithm
\algrenewcomment[1]{\(\triangleright\) #1}
\begin{algorithm}
\caption{}
\begin{algorithmic}
\Function{ISO-SPLIT}{$\{y_1,\dots,y_N\}$,$\alpha$}
\State $\{L_1,\dots,L_N\} \gets \text{InitializeLabels} (\{y_1,\dots,y_N\})$
\State $\text{UsedPairs} \gets \{\}$
\Loop
	\State $[k_1,k_2,\text{exists}] \gets \text{FindClosestPair}(\{y_1,\dots,y_N\},\{L_1,\dots,L_N\},\text{UsedPairs})$
	\If{$\text{not }\text{exists}$} \Comment{Converged}
		\State $\text{\textbf{break}}$ 
	\EndIf
	\State $S_1=\{j: L_j=k_1\}$
	\State $S_2=\{j: L_j=k_2\}$
	\State $V \gets \text{GetProjectionDirection}(S_1,S_2)$
	\State $X_1 \gets \{\text{Project}(V,y_j): j\in S_1\}$
	\State $X_2 \gets \{\text{Project}(V,y_j): j\in S_2\}$
	\State $[c,p] \gets \text{ComputeOptimalCutpoint}(X_1\cup X_2)$
	\If{$p<\alpha$} \Comment{Redistribute according to cutpoint}
		\ForAll{$j \in S_1\cup S_2$} 
			\If{$\text{Project}(L,y_j)\leq c$}
				$L_j \gets k_1$
			\Else
				$\text{ }L_j \gets k_2$
			\EndIf
		\EndFor
	\Else \Comment{ Merge}
		\State $L_j\gets k_1 \text{ }\forall j\in S_1\cup S_2$ 
	\EndIf
	\State $\text{UsedPairs}\gets\text{UsedPairs}\cup\{(S_1,S_2),(S_2,S_1)\}$
        \EndLoop
\State\Comment{Reassign labels to be in $\{1,\dots,K\}$}
\State $L\gets \text{Remap}(L)$
\State \Return $L$
\EndFunction
\end{algorithmic}
\label{alg:main_algorithm}
\end{algorithm}
\spacingset{1.45} % DON'T change the spacing!

The best line of projection may be chosen in various ways. The simplest approach is to use the line connecting the centroids of the two clusters of interest (this is what we used for the present study). Although this choice may be sufficient in most situations, the optimal hyperplane of separation may not be orthogonal to this line. Another approach (more computationally demanding) is to use a support vector machine to determine the orientation of the optimal hyperplane. The function \emph{GetProjectionDirection} in Algorithm~\ref{alg:main_algorithm} returns a unit vector $V$ representing the direction of the projection line, and the function \emph{Project} simply returns the inner product of this vector with each data point.

Similarly, there are various approaches for choosing the closest pair of clusters at each iteration (\emph{FindClosestPair}). One way is to minimize the distance between the two cluster centroids. Note, however, that we don't want to repeat the same 1D kernel operation more than once. Therefore, the closest pair that has not yet been handled may be chosen.

%\footnote{For reasons of computational efficiency, we used a slightly different method in this study, by selecting one cluster at random and then pairing it with the closest available cluster. We found this to reduce the number of iterations required for convergence and did not seem to have a significant impact on the output, although the effect was not systematically studied.}

The function \emph{InitializeLabels} creates an initial labeling (or partition) of the data. This may be implemented using the $k$-means algorithm with the number of initial clusters chosen to be much larger than the expected number of clusters in the dataset, the assumption being that the output should not be sensitive once $K_\text{initial}$ is large enough (see Appendix \ref{appendixSensitivity}). For our tests we used the minimum of $20$ and four times the true number of clusters. Since datasets may always be constructed such that our choice of $K_\text{initial}$ is not large enough, we will seek to improve this initialization step in future work.

The critical step is \emph{ComputeOptimalCutpoint}, which is the 1D clustering procedure described in the previous section.

%\begin{figure}
%\begin{center}
%\includegraphics[width=5.5in]{isosplit_vs_kmeans_01.png}
%\end{center}
%\caption{
%K-means assumes that the cluster populations are all the same. In this example (corresponding to Isotropic Simulation below), two relatively small clusters are merged while a larger cluster is split by k-means. On the other hand, ISO-SPLIT makes no assumptions about relative cluster sizes and is able to handle this situation with 99\% accuracy. Gaussian mixture clustering also fails in this particular case. Note that unlike the other two algorithms, ISO-SPLIT was not given information about the true number of clusters. Numbers inside parentheses are classification accuracies as defined in the text.
%}
%\label{fig:isosplit_vs_kmeans_01}
%\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{simulation2.eps}
\end{center}
\caption{
K-means assumes that the cluster populations and variances are all the same. In this example (corresponding to \textbf{Simulation 2 (Anisotropic)} below), two relatively small clusters are merged by k-means while a larger cluster is split. On the other hand, ISO-SPLIT makes no assumptions about relative cluster sizes and is able to handle this situation. Gaussian mixture clustering with unknown K (determined using BIC) also fails in this case. DBSCAN fails because the clusters do not all have the same density. Note that ISO-SPLIT was not given information about the true number of clusters or the expected density.
}
\label{fig:simulation2}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{example_dbscan.eps}
\end{center}
\caption{
Unlike ISO-SPLIT, DBSCAN can handle the non-convex situation where clusters cannot be separated by hyperplanes. However, the scale parameter $\epsilon$ must be properly chosen.
}
\label{fig:example_dbscan}
\end{figure}

Figure \ref{fig:simulation2} highlights a case where ISO-SPLIT outperforms k-means, DBSCAN, and GMM (for unknown $K$). This example was selected from Simulation 2 as will be described in Section~\ref{algorithm_comparison}. Unlike k-means and DBSCAN, ISO-SPLIT makes no assumptions about relative cluster population sizes and peak densities. On the other hand, Figure \ref{fig:example_dbscan} illustrates a case where DBSCAN (when properly tuned) performs better the other methods.

\begin{figure}
\begin{center}
\includegraphics[width=2.5in]{nongaussian_histogram.eps}
\end{center}
\caption{
Histogram of the 1D probability distribution used to generate skewed, non-Gaussian clusters in Simulation 3. See Equation \eqref{eq:nongaussian}.
}
\label{fig:nongaussian_histogram}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{simulation1.eps}
\end{center}
\caption{
Example run from \textbf{Simulation 1 (Isotropic)}. All investigated algorithms perform well when clusters are well-spaced and isotropic with equal variances and populations.
}
\label{fig:simulation1}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{simulation3.eps}
\end{center}
\caption{
Example run from \textbf{Simulation 3}, with non-Gaussian (skewed) clusters. ISO-SPLIT is density based and able to handle this situation, whereas k-means and GMM fail due to violation of their underlying assumptions. Despite being density based, DBSCAN also fails since it cannot handle datasets containing clusters of widely differing densities.
}
\label{fig:simulation3}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{simulation4.eps}
\end{center}
\caption{
Example run from \textbf{Simulation 4} in which clusters are tightly packed ($z_0=1.7$). The two density based methods (ISO-SPLIT and DBSCAN) erroneously merge adjacent clusters in this case because they are not sufficiently separated by density.
}
\label{fig:simulation4}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{simulation5.eps}
\end{center}
\caption{
Projection onto two dimensions of example run from \textbf{Simulation 5} with number of dimensions equal to $n=6$. ISO-SPLIT performs well in this case whereas all other algorithms struggle apart from GMM when given the correct $K$.
}
\label{fig:simulation5}
\end{figure}

\newcommand{\multicell}[2][c]{%
  \begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}

\spacingset{1.0} % Change spacing for table
\begin{table}
  \centering
\begin{tabular}{l|c|c|c|}
	 & \textbf{3 Clusters} & \textbf{6 Clusters} & \textbf{12 Clusters} \\
	\multicell{\textbf{Simulation 1 (Isotropic)}\\Gaussian, Isotropic, Equal pops.}  & & & \\ 
	\hline
	ISO-SPLIT & $98.6 \pm 0.2\%$ & $98.2 \pm 0.0\%$ & $98.1 \pm 0.0\%$ \\
	K-means (K known) & $99.1 \pm 0.0\%$ & $98.7 \pm 0.0\%$ & $98.6 \pm 0.0\%$ \\
	Gaussian Mixture (K known) & $99.0 \pm 0.0\%$ & $98.7 \pm 0.0\%$ & $98.2 \pm 0.2\%$ \\
	Gaussian Mixture (BIC) & $99.0 \pm 0.0\%$ & $98.7 \pm 0.0\%$ & $98.1 \pm 0.2\%$ \\
	DBSCAN ($\epsilon = 0.27$) & $94.9 \pm 0.1\%$ & $94.7 \pm 0.1\%$ & $94.5 \pm 0.1\%$ \\
	& & & \\
	\multicell{\textbf{Simulation 2 (Anisotropic)}\\Gaussian, \textbf{Anisotropic, Unequal pops.}}  & & & \\  
	\hline
	ISO-SPLIT & $92.5 \pm 1.2\%$ & $92.8 \pm 0.8\%$ & $96.7 \pm 0.3\%$ \\
	K-means (K known) & $84.6 \pm 1.6\%$ & $78.5 \pm 1.2\%$ & $83.7 \pm 0.7\%$ \\
	Gaussian Mixture (K known) & $97.9 \pm 0.5\%$ & $94.2 \pm 0.8\%$ & $90.9 \pm 0.7\%$ \\
	Gaussian Mixture (BIC) & $91.7 \pm 0.9\%$ & $93.0 \pm 0.7\%$ & $91.8 \pm 0.5\%$ \\
	DBSCAN ($\epsilon = 0.3$) & $70.6 \pm 1.9\%$ & $66.7 \pm 1.4\%$ & $66.5 \pm 1.3\%$ \\
	& & & \\
	\multicell{\textbf{Simulation 3 (Skewed)}\\\textbf{Non-Gaussian}, Anisotropic, Unequal pops.}  & & & \\ 
	\hline
	
	ISO-SPLIT & $91.5 \pm 1.1\%$ & $96.1 \pm 0.1\%$ & $94.5 \pm 0.2\%$ \\
	K-means (K known) & $84.3 \pm 1.5\%$ & $88.3 \pm 0.9\%$ & $81.5 \pm 0.7\%$ \\
	Gaussian Mixture (K known) & $79.8 \pm 1.0\%$ & $87.9 \pm 0.6\%$ & $81.6 \pm 0.5\%$ \\
	Gaussian Mixture (BIC) & $80.4 \pm 0.7\%$ & $82.2 \pm 0.6\%$ & $85.4 \pm 0.5\%$ \\
	DBSCAN ($\epsilon = 0.3$) & $78.0 \pm 1.6\%$ & $76.7 \pm 1.5\%$ & $77.4 \pm 0.9\%$ \\
	& & & \\
	\multicell{\textbf{Simulation 4 (Packed)}\\Gaussian, Isotropic, Equal pops., \textbf{Tightly packed}} & & & \\ 
	\hline
	ISO-SPLIT & $86.3 \pm 1.4\%$ & $78.8 \pm 1.9\%$ & $62.9 \pm 2.7\%$ \\
	K-means (K known) & $93.7 \pm 0.1\%$ & $91.6 \pm 0.1\%$ & $90.4 \pm 0.1\%$ \\
	Gaussian Mixture (K known) & $93.3 \pm 0.1\%$ & $90.9 \pm 0.1\%$ & $88.8 \pm 0.2\%$ \\
	Gaussian Mixture (BIC) & $93.3 \pm 0.1\%$ & $91.0 \pm 0.1\%$ & $88.5 \pm 0.3\%$ \\
	DBSCAN ($\epsilon = 0.3$) & $33.3 \pm 0.0\%$ & $16.7 \pm 0.0\%$ & $8.3 \pm 0.0\%$ \\
	& & & \\
	\multicell{\textbf{Simulation 5 (High-dimensional)}\\Gaussian, Isotropic, Equal pops., \textbf{\# Dims = 6}}  & & & \\ 
	\hline
	ISO-SPLIT & $85.6 \pm 1.5\%$ & $96.1 \pm 0.1\%$ & $93.6 \pm 0.4\%$ \\
	K-means (K known) & $77.8 \pm 1.9\%$ & $82.0 \pm 1.2\%$ & $76.6 \pm 0.9\%$ \\
	Gaussian Mixture (K known) & $98.0 \pm 0.6\%$ & $96.7 \pm 0.7\%$ & $88.4 \pm 0.9\%$ \\
	Gaussian Mixture (BIC) & $75.5 \pm 1.0\%$ & $76.8 \pm 0.8\%$ & $86.5 \pm 0.6\%$ \\
	DBSCAN ($\epsilon = 1.5$) & $49.0 \pm 2.0\%$ & $45.9 \pm 2.1\%$ & $43.7 \pm 2.0\%$ \\
	\hline
\end{tabular}
\caption{
\label{table:simulations}
Average accuracies over $20$ trial simulations with standard errors. All data, except for in Simulation 5, were generated in two dimensions ($n=2$). \textbf{Simulation 1}: Populations were $500$ per cluster, $\xi=0$, $\zeta=0$, and cluster separations of $z_0=2.5$ (see text for details). \textbf{Simulation 2}: Populations were randomly chosen between $100$ and $1000$ with $\xi=1.2$, $\zeta=2$, and cluster separations of $z_0=2.5$. \textbf{Simulation 3}: Same as Simulation 2 except with non-Gaussian/skewed clusters (see text for details). \textbf{Simulation 4}: Same as Simulation 1 except with clusters chosen closer together using $z_0=1.7$. \textbf{Simulation 5}: Same as the Simulation 2 except with a higher number of dimensions $n=6$.
}
\end{table}
\spacingset{1.45} % DON'T change the spacing!

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{simulation_result_graph.eps}
\end{center}
\caption{
Results of simulations from Table \ref{table:simulations} with $6$ true clusters. Accuracies for ISO-SPLIT are at least comparable to the other algorithms in all cases except for when clusters are tightly packed (Simulation 4). ISO-SPLIT yields the highest accuracy in the non-Gaussian clusters case (Simulation 3). ISO-SPLIT and GMM (BIC) were the only algorithms for which no adjustable parameters were specified.
}
\label{fig:result_bars}
\end{figure}

\section {Algorithm comparison via simulation}
\label{algorithm_comparison}

A series of experiments were performed to compare the various algorithms considered in this paper. The accuracy measure, related to the F-measure \cite[Ch.~17]{zaki-book}, is defined as 
$$a=\frac{1}{K}\sum_{c=1}^K \min\left(\frac{n_{c,\pi(c)}}{n_{c,*}},\frac{n_{c,\pi(c)}}{n_{*,\pi(c)}}\right),$$
where $K$ is the true number of clusters, $n_{i,j}$ is the number of points in true class $i$ labeled by the algorithm as class $j$, $n_{i,*}=\sum_j n_{i,j}$, $n_{*,j}=\sum_i n_{i,j}$, and $\pi(c)$ is the class number in the second labeling matching most closely to class $c$ in the true labeling (i.e., maximizing $n_{c,\pi(c)}$).
The average is taken over all clusters (rather than datapoints) so that the larger or denser clusters do not contribute with greater weight. Note also that the contribution of each cluster is the minimum of (a) the extent to which the cluster is not split, and (b) the extent to which the cluster is not merged with another cluster. Thus a contribution of $100\%$ means that the cluster is labeled perfectly with respect to both sensitivity and specificity.

Simulations in two dimensions and higher were performed by generating random samplings from a mixed multivariate Gaussian distribution (except for the Skewed simulation) with clusters corresponding to the individual Gaussian subpopulations. The centers, spreads, orientations, anisotropies, and populations of the Gaussians were varied randomly. Specifically, the random covariance matrix for each cluster was defined as
\[
\Sigma=R
\left(
\begin{array}{ccc}
e^{r_0\zeta + r_1\xi} & & 0 \\
 & \ddots &  \\
 0 &  & e^{r_0\zeta + r_n\xi} \\
\end{array}
\right)R^T,
\]
where $r_0,r_1,\dots,r_n$ are random numbers uniformly selected from $[-1,1]$, $\zeta$ is the spread variation factor, $\xi$ is the anisotropy variation factor and $R$ is a random rotation matrix. The cluster locations were chosen such that the clusters were packed tightly with the constraint that the solid ellipses corresponding to Mahalanobis distance $z\leq z_0$ did not intersect (see Appendix \ref{appendixPacking} for details). In Simulation 3 the Gaussian distributions were replaced by non-Gaussian distributions that were skewed in both dimensions. Specifically, the data points (prior to adjustment for the covariance matrix) were generated as $\tilde{R}[F(z_1),F(z_2)]$ for $z_j$ normally distributed with
\begin{equation}
\label{eq:nongaussian}F(z)=\log(|z+3|),
\end{equation}
and $\tilde{R}$ a random rotation matrix (fixed for each cluster). The histogram for the 1D non-Gaussian distribution is shown in Figure \ref{fig:nongaussian_histogram}, and examples of two-dimensional clusters are shown in Figure \ref{fig:simulation3}.

All experiments were performed on a Linux laptop with a 2.8GHz quad-core
i7 processor and 8GB RAM.
ISO-SPLIT was implemented in MATLAB with kernel routines in C++
(a URL for our software is given in Sec.~\ref{s:conc}).
The split threshold was $\alpha=0.1$ (see the discussion below).
The other clustering methods were implemented as follows.
We used a MATLAB implementation%
\footnote{Specifically we used the code \cite{kmeanspp_sorber} corrected to use
  the original ``$D^2$ weighting'' recommended in \cite{kmeanspp}.}
 of k-means++
 with $100$ trials/restarts. GMM was performed using \cite{vlfeat} in MATLAB using EM iterations with 20 restarts. Both k-means and GMM had the advantage of being provided with the true number of clusters as input. A second run of GMM was used to automatically select the number of clusters using the Bayesian information criterion (BIC) \cite{BIC}. We used a multi-core C++ implementation of DBSCAN \cite{dbscan_dbp}, with $\epsilon$ tuned by hand for each simulation to yield the highest average accuracy measure.

We now summarize the results.
 All methods performed well in the Isotropic simulation under the conditions of equal populations, equal variances, and isotropic clusters, the ideal scenario for k-means (Figure \ref{fig:simulation1}).
For the second simulation, ISO-SPLIT and GMM (with known $K$) performed best. As expected, k-means did not do as well since the clusters were not isotropic and had unequal populations. The unequal cluster densities caused problems for DBSCAN. See Figure \ref{fig:simulation2}.
As expected, GMM did less well in the Skewed simulation, whereas ISO-SPLIT handled this situation without difficulty, as illustrated in Figure \ref{fig:simulation3}.
The number of dimensions was increased to $6$ in the fifth simulation. In this case ISO-SPLIT performed at least as well as the other algorithms. DBSCAN particularly struggled in this higher dimensional case. When $K$ was not known, GMM also struggled as illustrated in Figure \ref{fig:simulation5}.

In general, we see from the overall summary in Figure~\ref{fig:result_bars} that ISO-SPLIT did as well or better than all other algorithms except in Simulation 4 for which clusters were tightly packed, presumably because the tighter clusters were no longer separated by regions of significantly lower density (see Figure \ref{fig:simulation4}).

\section {Application to spike sorting data}
\label{s:spike}

The algorithms considered in this paper were applied to spike sorting of neural electrophysiological signals, that is, the clustering of spiking events in a time series into $K$ clusters that can often be associated with individual neurons. Clustering is a key step in spike sorting. More specifically, we took 7 channels of data from a set of adjacent electrodes in a multielectrode array which records voltages from an {\em ex vivo} monkey retina \cite{litke}; the 2 minutes of 20 kHz sampled time series data was supplied to us by the Chichilnisky Lab at Stanford. A set of points in $\mathbb{R}^{10}$ that needed to be clustered was created as follows. Firstly the time series was high-pass filtered at 300 Hz, then windows of time series of length 60 samples were extracted in which the minimum voltage dropped below $-120 \mu$V. (We also applied an automatic procedure to remove windows that did not appear to be single spike events.) This gave 7275 windows. The data in each window was upsampled by a factor of 3 using Hann-windowed sync interpolation, then negative-peak aligned to the central time point, then stacked to give a length-1470 column vector. Dimension reduction to 10 dimensions was finally done using PCA; that is, the 1470-by-7275 matrix $A$ was replaced by the first 10 columns of $V^T A$, where $V$ is the matrix whose columns are the eigenvectors of $AA^T$ ordered by descending eigenvalue. Columns of the resulting $A$ gave the points to cluster.

The resulting clustering (neuron labelings) are shown in Figure \ref{fig:real_data_1} with the centroid waveforms for each cluster displayed in Figure \ref{fig:real_data_2}. The adjustable parameters for k-means, GMM, and DBSCAN were carefully chosen to yield the same number of clusters ($K=8$) as ISO-SPLIT. Each of the algorithms produced a splitting into into qualitatively distinct centroid waveforms, suggesting that at least 8 detectable neural units are present. Various validation methods, and human judgements, are now commonly applied to determine whether each cluster is a single neural unit, e.g., \cite{Hill2011}; we do not attempt to perform that here.

We emphasize that ISO-SPLIT required no adjustment of free parameters, in contrast to all but the (poorly-performing) GMM with BIC. Indeed, when the BIC was used to select $K$ in GMM, many more clusters were identified, most of them duplicates, or artificially split clusters (see Figure \ref{fig:real_data_2}). An additional run of DBSCAN with $\epsilon$ reduced from $70$ to $60$ identified an additional cluster, demonstrating the sensitivity of the output to adjustments in this parameter. Further analysis is required to determine which results are closer to the truth.

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{real_data_1.eps}
\end{center}
\caption{
  Spike sorting on real data. The first three dimensions of the 10-dimensional PCA feature space are shown, with coloring indicating cluster (neuronal unit) membership produced by each clustering algorithm indicated. The classifications were highly consistent between the various algorithms when k-means, GMM, and DBSCAN were tuned to yield the same number of clusters as ISO-SPLIT.
  The red dotted lines indicate axes intersecting at the origin.
}
\label{fig:real_data_1}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{real_data_2.eps}
\end{center}
\caption{
Spike waveforms corresponding to the centroids of the clusters in Figure \ref{fig:real_data_1}. Rows represent electrode channels and columns represent distinct neurons/clusters.
}
\label{fig:real_data_2}
\end{figure}


\section {Calibration of ISO-SPLIT using Monte Carlo simulation}

\label{monte-carlo}

In this section we discuss how we calibrate the statistical test for whether to reject the null hypothesis that $N$ samples come from a single unimodal 1D distribution, described at the end of Section~\ref{clustering_1d}. We estimate using Monte Carlo simultion the parameters $\mu_{N,m}$ and $\sigma_{N,m}$ for various $N$ and $m$ in the following manner; these values are used to build a lookup table for rapid future evaluations. We take as the null hypothesis a uniform distribution on [-1,1] \footnote{It might be possible to improve the results by taking, for example, a Gaussian as the null hypothesis. However, we observe that our calibration depends little on the form of the null distribution, as long as it is unimodal; this is because the updown regression step removes trends and leaves an approximately constant density.}. A further set of Monte Carlo simulations were then used to convert $\eta(\tilde{E})$ to a $p$-value.

As shown in Figures \ref{fig:calibration_curves_01} and \ref{fig:calibration_curves_02}, $\mu_{N,m}$ is always an increasing function of $m$ approaching zero as $m$ becomes large. On the other hand, it is a decreasing function of $N$ because as the sample size becomes larger, there is an increased chance of a randomly occuring larger dip in density. The standard deviation $\sigma_{N,m}$ tends to decrease with increasing $m$ and increase with increasing $N$.

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{calibration_curves_01.eps}
\end{center}
\caption{
Calibration curves obtained using Monte Carlo simulation with 1000 sample sets for each $N$ taken from a uniform distribution. The discontinuity at $N=1000$ corresponds to the choice $m_\text{max}=1000$.
}
\label{fig:calibration_curves_01}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{calibration_curves_02.eps}
\end{center}
\caption{
Calibration curves obtained using Monte Carlo simulation with 1000 sample sets for various $N$ taken from a uniform distribution.
}
\label{fig:calibration_curves_02}
\end{figure}

\spacingset{1.0} % Change spacing for table
%\def\arraystretch{1}
\begin{table}
  \centering
  \begin{tabular}{l|c|c|c|}
	 & \textbf{3 Clusters} & \textbf{6 Clusters} & \textbf{12 Clusters} \\
	\multicell{\textbf{Simulation 1 (Isotropic)}\\Gaussian, Isotropic, Equal pops.}  & & & \\ 
	\hline
	ISO-SPLIT & $0.05$ & $0.10$ & $0.32$ \\
	K-means (K known) & $0.17$ & $0.54$ & $2.12$ \\
	Gaussian Mixture (K known) & $0.03$ & $0.20$ & $1.20$ \\
	Gaussian Mixture (BIC) & $0.17$ & $1.08$ & $9.66$ \\
	DBSCAN ($\epsilon = 0.27$) & $0.09$ & $0.10$ & $0.10$ \\
	& & & \\
	\multicell{\textbf{Simulation 2 (Anisotropic)}\\Gaussian, \textbf{Anisotropic, Unequal pops.}}  & & & \\  
	\hline
	ISO-SPLIT & $0.05$ & $0.08$ & $0.56$ \\
	K-means (K known) & $0.22$ & $0.54$ & $2.65$ \\
	Gaussian Mixture (K known) & $0.05$ & $0.15$ & $1.56$ \\
	Gaussian Mixture (BIC) & $0.30$ & $0.94$ & $13.93$ \\
	DBSCAN ($\epsilon = 0.3$) & $0.08$ & $0.09$ & $0.11$ \\
	& & & \\
	\multicell{\textbf{Simulation 3 (Skewed)}\\\textbf{Non-Gaussian}, Anisotropic, Unequal pops.}  & & & \\ 
	\hline
	ISO-SPLIT & $0.06$ & $0.20$ & $0.59$ \\
	K-means (K known) & $0.23$ & $0.85$ & $2.28$ \\
	Gaussian Mixture (K known) & $0.05$ & $0.31$ & $1.35$ \\
	Gaussian Mixture (BIC) & $0.60$ & $4.82$ & $16.74$ \\
	DBSCAN ($\epsilon = 0.3$) & $0.09$ & $0.10$ & $0.11$ \\
	& & & \\
	\multicell{\textbf{Simulation 4 (Packed)}\\Gaussian, Isotropic, Equal pops., \textbf{Tightly packed}} & & & \\
	\hline
	ISO-SPLIT & $0.06$ & $0.18$ & $0.93$ \\
	K-means (K known) & $0.22$ & $0.76$ & $3.20$ \\
	Gaussian Mixture (K known) & $0.07$ & $0.41$ & $2.14$ \\
	Gaussian Mixture (BIC) & $0.28$ & $2.05$ & $16.21$ \\
	DBSCAN ($\epsilon = 0.3$) & $0.09$ & $0.10$ & $0.10$ \\
	& & & \\
	\multicell{\textbf{Simulation 5 (High-dimensional)}\\Gaussian, Isotropic, Equal pops., \textbf{\# Dims = 6}}  & & & \\ 
	\hline
	ISO-SPLIT & $0.06$ & $0.46$ & $3.04$ \\
	K-means (K known) & $0.37$ & $1.23$ & $3.07$ \\
	Gaussian Mixture (K known) & $0.04$ & $0.19$ & $0.61$ \\
	Gaussian Mixture (BIC) & $0.43$ & $2.90$ & $7.42$ \\
	DBSCAN ($\epsilon = 1.5$) & $0.09$ & $0.11$ & $0.12$ \\
\end{tabular}
\caption{
\label{table:simulations2}
Average computation times in seconds per trial run for the simulations of Table \ref{table:simulations}.
}
\end{table}
\spacingset{1.45} % DON'T change the spacing!


\section {Computational efficiency}

Each iteration of ISO-SPLIT comprises two steps. The first step, projection onto 1D space, has computation time $O(N_0 n)$ where $n$ is the number of dimensions and $N_0<N$ is the number of points involved in the two clusters of interest. The second step is 1D clustering using isotonic regression and has time complexity $O(N_0)$. Due to the complexity of the cluster redistributions at each step, it is difficult to theoretically estimate the number iterations required for convergence.

Table \ref{table:simulations2} shows empirical average computation times for the simulations of Table \ref{table:simulations}. Overall prefactors in the running times should not be given too much meaning, since they are highly implementation-dependent. In almost every case, GMM with unknown $K$ took the longest since the algorithm needed to be run several times to find the optimal number of clusters using the BIC. Even when $K$ was known, GMM and k-means was rerun many times (20 for GMM and 100 for k-means), and therefore took longer on average than ISO-SPLIT in almost every case. With a highly optimized C++ implementation \cite{dbscan_dbp}, DBSCAN was the most efficient algorithm of those considered. There was a significant increase in ISO-SPLIT's run time from 6 to 12 clusters, but it should be noted that both $K$ and $N$ were increased by a factor of $2$, since the number of points per cluster was fixed.



Future work will investigate the theoretical bounds on the number of iterations required for convergence. Here we present empirical estimates for the scaling properties with increasing values of $N$ (the number of samples), $n$ (the number of dimensions), and $K_\text{true}$ (the true number of clusters). The most time-consuming step in each iteration (isotonic regression) is independent of both $n$ and $K_\text{true}$ but the unknown quantity is the number of iterations required for convergence. Figure \ref{fig:computation_times_01} shows the results of three simulations suggesting that computation time scales linearly with all three simulation parameters.

Using code profiling in MATLAB, we found that the majority of time (approx. 90\%) was spent performing 1D clustering (Section~\ref{clustering_1d}) as compared with finding the best pair of clusters to compare, computing centroids, and projecting data onto lines. Of course this figure will depend on $n$, $K$, $N$, and the structure of the data.

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{computation_times_01.eps}
\end{center}
\caption{
Empirical dependence of average computation time on three simulation parameters, averaged over $20$ repeats; (A) number of points per sample, with number of true clusters fixed at $6$ and $n$ fixed at $2$; (B) number of true clusters, with $N$ fixed at $1000$ and $n$ fixed at $2$; (C) number of dimensions with $N$ fixed at $1000$ and number of true clusters fixed at $6$, with noise dimensions added past the first two dimensions.
}
\label{fig:computation_times_01}
\end{figure}

As mentioned in the introduction, pdfCluster \cite{pdfcluster} is also a non-parametric, density-based algorithm. A probability density function is constructed from the estimated density of datapoints. The clustering is then performed using this continuous function of space, and does not depend on the hyperplane-separation assumption required by ISO-SPLIT. However, empirically we found the R implementation \cite{pdfclusterR} to be time-consuming compared with ISO-SPLIT for larger datasets. To compare run times, we ran the two algorithms three times using the parameters of Simulation 2, with 3, 6, and 12 clusters (N = 2400, 4200, 7800). For ISO-SPLIT, the run times were 0.32, 0.44, and 2.1 seconds, respectively. The classification output from pdfCluster (with default parameters) qualitatively matched the output from ISO-SPLIT, but the computation times were 8.9, 31, and 180 seconds, respectively. The discrepancy was more dramatic when increasing to six dimensions where pdfCluster had 380 seconds of processing time for only 3 clusters, compared with less than 1 second for ISO-SPLIT.

\section {Discussion}

We have shown that, for the target application of spike sorting, our new algorithm produces results that are consistent with those of standard clustering algorithms (k-means, GMM, DBSCAN). Yet the key advantage of ISO-SPLIT is that it does not require selection of scale parameters nor of the number of clusters. This is very important in situations where manual processing steps are to be avoided.
%for example when minimizing human bias or in order to increment repeatability.
Automation is also critical when hundreds of clustering runs must be executed within a single analysis, e.g., applications of spike sorting with large numbers of electrodes. Furthermore, the accuracy of ISO-SPLIT appears to exceed that of standard algorithms in the context of many simulations performed in this study. Most notably, it excels when clusters are non-Gaussian with varying populations, orientations, spreads, and anisotropies.

While ISO-SPLIT outperforms standard algorithms in situations satisfying the assumptions of the method, the algorithm has general limitations and is not suited for all contexts. Because ISO-SPLIT depends on statistically significant density dips between clusters, erroneous merging occurs when clusters are positioned close to one another (see the Packed simulation). Certainly this is a challenging scenario for all algorithms, but k-means or mixture models are better suited to handle these cases. On the other hand,
if the underlying density has dips which separate clusters, ISO-SPLIT will
find them for sufficiently large $N$.

Our theory depends on the assumption that the data arise from a continuous probability distribution. While no particular noise model is assumed, we do assume that, after projection onto any 1D space, the distribution is locally well approximated by a uniform distribution. This condition is satisfied for any smooth probability distribution. In particular, this guarantees that no two samples have exactly the same value (which could lead to an infinite estimate of pointwise density). Situations where values are drawn from a discrete grid (e.g., an integer lattice) will fail to have this crucial property. One remedy for such scenarios could be to add random offsets to the datapoints to form a continuous distribution.

Clusters with non-convex shapes may be well separated in density but not separated by a hyperplane (Figure \ref{fig:example_dbscan}). In these situations, alternative methods such as DBSCAN are preferable. But even when clusters are convex, a pair may be oriented such that the separating hyperplane is not orthogonal to the line connecting the centroids.

While each iteration is efficient (essentially linear in a subset of the number of points of interest), computation time may be a concern since the number of iterations required to converge is unknown. Empirically, total computation time appears to increase linearly with the number of clusters, the number of dimensions, and the sample size.

As mentioned above, a principal advantage of ISO-SPLIT is that it does not require parameter adjustments. Indeed, the core computational step is isotonic regression, which does not rely on any tunable parameters. Three parameters are fixed once and for all, namely $\alpha$, the significance level for splitting clusters, $K_\text{initial}$ the initial number of clusters, and $m_\text{max}$ used in the test statistic. In Appendix \ref{appendixSensitivity} we argue that the algorithm is not sensitive to these values over reasonable ranges. It may seem like the choice of $\alpha=0.1$ may be too high since it implies that split clusters will erroneously stay split approximately $10\%$ of the time. In practice, however, subsets typically have several chances to be merged during the course of the iterations, and a true cluster rarely remains split by the time of convergence (see Appendix \ref{appendixSensitivity}).


\section{Conclusion}
\label{s:conc}

A multi-dimensional clustering algorithm, ISO-SPLIT, based on density clustering of one-dimensional projections was introduced. The algorithm was motivated by the electrophysiological spike sorting application. Unlike many existing techniques, the new algorithm does not depend on adjustable parameters such as scale or \emph{a priori} knowledge of the number of clusters. Using simulations, ISO-SPLIT was compared with k-means, Gaussian mixture, and DBSCAN, and was shown to outperform these methods in situations where clusters were separated by regions of relatively lower density and where each pair of clusters could be largely split by a hyperplane. ISO-SPLIT was especially effective for non-Gaussian cluster distributions. Future research will focus on applying the algorithm to additional real-world problems as well as improving computational efficiency.

A MATLAB/C++ implementation of ISO-SPLIT is freely available
at the following URL:\\
{\tt http://github.com/magland/isosplit}


\section*{Acknowledgments}

We have benefited from useful discussions with Leslie Greengard,
Marina Spivak, Bin Yu, and Cheng Li.
We are grateful for EJ Chichilnisky and his research group
for supplying us with the retinal neural recording data used
in section~\ref{s:spike}.



\appendix % AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA


\algrenewcomment[1]{\(\triangleright\) #1}

\spacingset{0.95} % Change spacing for algorithm
\begin{algorithm}
\caption{}
\begin{algorithmic}
\Function{UpDownIsotonic}{x}
	\State \Comment{Isotonic regression for increasing followed by decreasing}
	\State $N \gets \text{length}(x)$
	\State $b \gets \text{FindOptimalB}(x)$
	\State $y^{(1)} \gets \text{IsotonicIncreasing}([x_1,\dots,x_b])$
	\State $y^{(2)} \gets \text{IsotonicDecreasing}([x_b,\dots,x_N])$
	\State $y \gets [y^{(1)}_1,\dots,y^{(1)}_b,y^{(2)}_2,\dots,y^{(2)}_{N-b+1}]$
	\State \Return $y$
\EndFunction
\Statex
\Function{FindOptimalB}{x}
	\Statex \Comment{Find where to switch direction}
	\State $x^{(1)} \gets x$
	\State $x^{(2)} \gets -\text{Reverse}(x)$
	\State $\mu^{(1)} \gets \text{PAVA-MSE}(x_1)$	
	\State $\mu^{(2)} \gets \text{Reverse}(\text{PAVA-MSE}(x^{(2)}))$
	\State Find $b$ to minimize $\mu^{(1)}_b+\mu^{(2)}_b$
	\State \Return $b$
\EndFunction
\Statex
\Function{Reverse}{$[x_1,\dots,x_N]$}
	\State \Comment{Reverse the ordering}
	\State \Return $[x_N,x_{N-1},\dots,x_1]$
\EndFunction
\Statex
\Function{PAVA-MSE}{$[x_1,\dots,x_N]$,$[w_1,\dots,w_N]$}          % ahb fixed {}
	\State \Comment{Modified PAVA to return MSE at every index}
	\State $i \gets 1$, $j \gets 1$
	\State $\text{count}[i] \gets 1$, $\text{wcount}[i] \gets w_j$
	\State $\text{sum}[i] \gets w_j x_j$, $\text{sumsqr}[i] \gets w_j x_j^2$
	\State $\mu_j \gets 0$
	\State
	\For{$j=2\dots N$}
		\State $i \gets i+1$
		\State $\text{count}[i] \gets 1$, $\text{wcount}[i] \gets w_j$
		\State $\text{sum}[i] \gets w_j x_j$, $\text{sumsqr}[i] \gets w_j x_j^2$
		\State $\mu_j \gets \mu_{j-1}$
		\Loop
			\If{$i=1$}
				\textbf{ break} 
			\EndIf
			\If{$\text{sum}[i-1]/\text{count}[i-1]<\text{sum}[i]/\text{count}[i]$}
				\textbf{ break} 
			\Else \Comment{ Merge the blocks}
				\State $\mu_{\text{before}}\gets\text{sumsqr}[i-1]-\text{sum}[i-1]^2/\text{count}[i-1]$
				\State $\mu_{\text{before}}\gets \mu_{\text{before}}+\text{sumsqr}[i]-\text{sum}[i]^2/\text{count}[i]$
				\State $\text{count}[i-1] \gets \text{count}[i-1]+\text{count}[i]$, $\text{wcount}[i-1] \gets \text{wcount}[i-1]+\text{wcount}[i]$
				\State $\text{sum}[i-1] \gets \text{sum}[i-1]+\text{sum}[i]$, $\text{sumsqr}[i-1] \gets \text{sumsqr}[i-1]+\text{sumsqr}[i]$
				\State $\mu_{\text{after}}\gets\text{sumsqr}[i-1]-\text{sum}[i-1]^2/\text{count}[i-1]$
				\State $\mu_j\gets \mu_j+\mu_{\text{after}}-\mu_{\text{before}}$
				\State $i\gets i-1$
			\EndIf
		\EndLoop
	\EndFor
	\State \Return $\mu$
\EndFunction

\end{algorithmic}
\label{alg:PAVA2}
\end{algorithm}
\spacingset{1.45} % DON'T change the spacing!



\section {Updown isotonic regression}
\label{appendixUpdown}

In this section we outline a computationally efficient variant of isotonic regression that provides the critical step in the kernel operation of ISO-SPLIT. Isotonic regression is a non-parametric method for fitting an ordered set of real numbers by a monotonically increasing (or decreasing) function. Suppose we want to find the best least-squares approximation of the sequence $x_1,\dots,x_N$ by a monotonically increasing sequence. Considering the more general problem that includes weights, we want to minimize the objective function
\begin{equation}
F(y)=\sum_{i=1}^N w_i(y_i-x_i)^2,
\label{eq:least_squares}
\end{equation}
subject to
$$y_1\leq y_2\leq\dots\leq y_N.$$
This may be solved in linear time using the pool adjacent violators algorithm (PAVA) \cite{pava}; we do not include the full pseudocode for this standard algorithm but note that it is essentially the same as PAVA-MSE in Algorithm~\ref{alg:PAVA2}.

As discussed above, ISO-SPLIT depends on a variant of isotonic regression, which we call \emph{updown isotonic regression}. In this case we need to find a turning point $y_b$ such that $y_1\leq y_2\leq\dots\leq y_b$ and $y_b\geq y_{b+1}\dots\geq y_N$. Again we want to minimize $F(y)$ of Equation~\eqref{eq:least_squares}. One way to solve this is to use an exhaustive search for $b\in\{1,\dots,N\}$. However, this would have $O(N^2)$ time complexity.

A modified PAVA that finds the optimal $b$ for the updown case in linear time is presented in Algorithm~\ref{alg:PAVA2}. The idea is to perform isotonic regression from left to right and then right to left using a modified algorithm where the mean-squared error is recorded at each step. The turning point is then chosen to minimize the sum of the two errors.

Downup isotonic regression is also needed by the algorithm. This procedure is a straightforward modification of updown in Algorithm~\ref{alg:PAVA2} by negating both the input and output.




\section {Sensitivity to parameters}
\label{appendixSensitivity}

In this work we have claimed that ISO-SPLIT does not require parameter adjustments which depend on the application or nature of the dataset, thus facilitating fully automated clustering. However, practically speaking, there are three values mentioned in this paper that need to be set. In this section we demonstrate that the algorithm is not sensitive to these choices provided that they fall within a reasonable range.

First, the significance level $\alpha$ needs to be chosen. For the above simulations we used $\alpha=0.1$ which may seem high since it implies that split clusters will erroneously stay split approximately $10\%$ of the time. In practice, however, subsets typically have several chances to be merged during the course of the iterations, and true clusters rarely remain split by the time of convergence. To demonstrate, we ran Simulation 2 again with varying $\alpha$. The results are found in Table \ref{table:alpha_dependence}. As $\alpha$ was increased, the accuracies remained virtually constant while the computation times increased, presumably because sub-clusters took more iterations to be merged for larger $\alpha$.


%alphas =    0.0010    0.0050    0.0100    0.0500    0.1000    0.2000    0.4000    0.8000    0.8500
%accs =    0.8024    0.8687    0.8746    0.9270    0.9250    0.9367    0.9406    0.9426    0.9380
%elapsed =    0.3105    0.3199    0.3207    0.3488    0.3447    0.3614    0.3992    0.5522    0.5644
  \begin{table}[t]
\centering
    \begin{tabular}{c|c|c|}
	\textbf{$\alpha$} & \textbf{Accuracy} & \textbf{Time (s)} \\
	\hline
	0.001 & 65\% & 0.32 \\
0.005 & 76\% & 0.35 \\
0.01 & 81\% & 0.42 \\
0.05 & 90\% & 0.37 \\
0.1 & 89\% & 0.5 \\
0.2 & 87\% & 0.46 \\
0.4 & 92\% & 0.51 \\
0.8 & 92\% & 0.74 \\
0.85 & 92\% & 0.72 \\
\end{tabular}
\caption{
\label{table:alpha_dependence}
Results of Simulation 2 (ISO-SPLIT, 6 clusters) repeated with varying $\alpha$ demonstrating insensitivity to the choice of this parameter (when within a reasonable range). Computation time increased for larger $\alpha$ presumably because more iterations were required for sub-clusters to be merged.
}
\end{table}

The second value to be set is $K_{\text{initial}}$, the number of clusters used to initialize the algorithm. Our hypothesis is that the choice of this parameter will have no affect on the output assuming that it is chosen large enough. This is supported in Table \ref{table:initial_K_dependence}. As discussed above, larger values of $K_{\text{initial}}$ will lead to significantly longer run times (quadratic time dependence), so it is important not to set this value to be much higher than necessary. 

%Ks = 3     6    12    24    48    96
%accs = 0.4814    0.8397    0.9097    0.9250    0.9246    0.9216
%elapsed = 0.0727    0.1214    0.2049    0.3409    1.0778    6.3507
\begin{table}[t]
  \centering
\begin{tabular}{c|c|c|}
	\textbf{Initial $K$} & \textbf{Accuracy} & \textbf{Time (s)} \\
	\hline
	3 & 45\% & 0.03 \\
6 & 92\% & 0.13 \\
12 & 86\% & 0.24 \\
24 & 85\% & 0.40 \\
48 & 87\% & 1.54 \\
96 & 89\% & 10.60 \\
\end{tabular}
\caption{
\label{table:initial_K_dependence}
Results of Simulation 2 (ISO-SPLIT, 6 clusters) repeated with varying $K_\text{initial}$ demonstrating insensitivity to the choice of this parameter (above a certain value). As expected, computation time increased for larger $K_\text{initial}$.
}
\end{table}

Finally, the $m_\text{max}$ of the test statistic needs to be selected. Again the hypothesis is that this parameter just needs to be selected to be large enough, and that our choice of $m_\text{max}=1000$ is sufficient for practical purposes. Evidence for this is provided in Table \ref{table:m_max_dependence}.

%m_maxs = 1           5          10          20          50         100         200         500        1000
%accs =    0.6306    0.8860    0.8947    0.9117    0.9359    0.9355    0.9250    0.9250    0.9250
%elapsed =    0.2894    0.3446    0.3270    0.3417    0.3488    0.3588    0.3450    0.3468    0.3501
\begin{table}[t]
  \centering
\begin{tabular}{c|c|c|}
	\textbf{$m_\text{max}$} & \textbf{Accuracy} & \textbf{Time (s)} \\
	\hline
1 & 19\% & 0.22 \\
5 & 29\% & 0.25 \\
10 & 52\% & 0.31 \\
20 & 65\% & 0.37 \\
50 & 87\% & 0.43 \\
100 & 92\% & 0.45 \\
200 & 87\% & 0.44 \\
500 & 87\% & 0.45 \\
1000 & 88\% & 0.42 \\
\end{tabular}
\caption{
\label{table:m_max_dependence}
Results of Simulation 2 (ISO-SPLIT, 6 clusters) repeated with varying $m_\text{max}$ demonstrating insensitivity to the choice of this parameter (above a certain value). The computation time does not appear to change for larger values suggesting that this parameter may be chosen to be conservatively large.
}od
\end{table}


\section {Packing Gaussian clusters for simulations}
\label{appendixPacking}

Simulations 1-5 required automatic generation of synthetic datasets with fixed numbers of clusters of varying densities, populations, spreads, anisotropies, and orientations. The most challenging programming task was to determine the random locations of the cluster centers. If clusters were spaced out too much then the clustering would be trivial. On the other hand, overlapping clusters cannot be expected to be successfully separated. Here we briefly describe a procedure for choosing the locations such that clusters are tightly packed with the constraint that the solid ellipsoids corresponding to Mahalanobis distance $z_0$ do not intersect. Thus $z_0$ is a constant controlling the tightness of packing, fixed for each simulation. In above simulations we considered $z_0=2.5\text{ and }1.7$.

The clusters are positioned iteratively, one at a time. Each cluster is positioned at the origin and then moved out radially in small increments of a random direction until the non-intersection criteria is satisfied. Thus we only need to determine whether two clusters defined by $(\mu_1,\Sigma_1)$ and $(\mu_2,\Sigma_2)$ are spaced far enough apart. Here $\mu_j$ are the cluster centers and $\Sigma_j$ are the covariance matrices. The problem boils down to determining whether two arbitrary $n$-dimensional ellipsoids intersect. Surprisingly this is a nontrivial task, especially in higher dimensions, but an efficient iterative solution was discovered by Lin and Han in 2002 \cite{ellipsoid-distance}. For the present study, the Lin-Han algorithm was implemented in MATLAB.


\bibliographystyle{Chicago}

\bibliography{isosplit}
\end{document}
